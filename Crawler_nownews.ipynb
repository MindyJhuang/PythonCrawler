{
 "metadata": {
  "name": "",
  "signature": "sha256:3b61b56bdb6b15268ccfd7a4b594c297e9af08ed8a67513d650f5cd6902ed8c6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse, requests, os, re, time\n",
      "from bs4 import BeautifulSoup\n",
      "from pymongo import Connection"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "link_pool_dic = {}    # \u66ab\u5b58\u9023\u7d50\u6e05\u55ae\u88e1\u7684\u9023\u7d50\n",
      "new_link_dic = {}    # \u66ab\u5b58\u6e05\u55ae\u9023\u7d50\u88e1\u9762\u6c92\u6709\u7684\u65b0\u9023\u7d50\n",
      "retry_dic = {}    # \u66ab\u5b58\u9023\u7d50\u5931\u6557\u7684\u9023\u7d50\n",
      "doc_id_dic = {}    # \u66ab\u5b58 mongoDB \u4e2d\u5df2\u5b58\u5728\u7684 document id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \u53d6\u65b0\u805e\u985e\u5225\u9023\u7d50\n",
      "def get_catalog_linklist (index_link, path):\n",
      "    page_format = 'http://www.nownews.com%s/'\n",
      "    result = requests.get(index_link) \n",
      "    response = result.text.encode('utf8')\n",
      "    soup = BeautifulSoup(response)\n",
      "    catalog_link = soup.findAll('li',{'class':'navtop-button'})[1:]\n",
      "    print 'start to get catalog links',\n",
      "    for num in catalog_link:\n",
      "        print '.',\n",
      "        catalog =  num.a['href']\n",
      "        catalog_url = page_format%catalog+'r/'\n",
      "        get_news_list(catalog_url, path)\n",
      "        #print catalog_url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \u53d6\u65b0\u805e\u9023\u7d50\u6e05\u55ae\n",
      "def get_news_list (catalog_url, path):\n",
      "    page_format = catalog_url+'%d'\n",
      "    file_name = catalog_url.split('cat/')[1].split('/r')[0]    \n",
      "    start = 1\n",
      "    print 'start to get news link list',\n",
      "    for page in range(start,2):\n",
      "        try:\n",
      "            print '.',        \n",
      "            content_link = page_format%start\n",
      "            result = requests.get(content_link)\n",
      "            response = result.text.encode('utf8')\n",
      "            soup = BeautifulSoup(response)\n",
      "            get_links = soup.find('ul',{'id':'result-list'}).findAll('h2')        \n",
      "            if len(get_links)>0:\n",
      "                for news_link in get_links:\n",
      "                    check_list (news_link, file_name, path)                \n",
      "            else:\n",
      "                break\n",
      "            start = start+1\n",
      "            time.sleep(5)\n",
      "        except:\n",
      "            print catalog_url\n",
      "            print 'stop at page' + str(page)\n",
      "    f = open(path+file_name+\"_list.txt\", 'w') \n",
      "    for  overwrite_link in new_link_dic:        \n",
      "        f.write(overwrite_link + \"\\n\")\n",
      "    f.close() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \u78ba\u8a8d\u9023\u7d50\u662f\u5426\u91cd\u8907\n",
      "def check_list (link, name, path):    \n",
      "    file_list = os.listdir(path)\n",
      "    filename = name + \"_list.txt\"\n",
      "    if filename not in file_list:\n",
      "        f= open(path+filename, \"w\") \n",
      "        f.write(link.a['href'] + \"\\n\")\n",
      "        f.close()\n",
      "    else:\n",
      "        f= open(path+filename, \"r\")\n",
      "        for line in f.readlines():\n",
      "            caseno =  line.strip().split('n/')[1]\n",
      "            link_pool_dic[caseno] = 1\n",
      "        f.close()   \n",
      "        check_links = link.a['href'].split('n/')[1]\n",
      "        if check_links not in link_pool_dic:\n",
      "            new_link_dic[link.a['href']] = 1 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \u91cd\u53d6\u9023\u7d50\n",
      "def retry_get_info():\n",
      "    for retry_link in retry_dic:\n",
      "            try:\n",
      "                result = requests.get(retry_link)\n",
      "                response = result.text.encode('utf8')\n",
      "                soup = BeautifulSoup(response)\n",
      "                title = soup.find('div',{'class':'news_story'}).find('h1').text\n",
      "                content = soup.find('div',{'class':'story_content'}).findAll('p')[1:-1]\n",
      "                time_clean = soup.find('div',{'id':'reporter_info'}).find('p').text.encode('utf8')\n",
      "                date = time_clean.strip().replace('\u5e74','/').replace('\u6708','/').replace('\u65e5','').replace('\\n','').replace(' ','')\n",
      "                keywords = content.text.replace('\\n',' ')\n",
      "                photo = soup.find('div',{'class':'autozoom'}).findAll('img')\n",
      "                photo_link = photo[0]['src']\n",
      "                data_insert(title, date, content, retry_link, filename, keywords, photo_link)\n",
      "                time.sleep(5) \n",
      "                print '.',\n",
      "            except:\n",
      "                print retry_link"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \u53d6\u5f97\u65b0\u805e\u5167\u5bb9\n",
      "def get_news_contents (path):\n",
      "    dirs = os.listdir(path)\n",
      "    print 'start to get news content',\n",
      "    for filename in dirs:\n",
      "        con=''\n",
      "        f = open(path+filename,'r')\n",
      "        for line in f.readlines(): \n",
      "            try:\n",
      "                news_link = line.strip()\n",
      "                result = requests.get(news_link)\n",
      "                response = result.text.encode('utf8')\n",
      "                soup = BeautifulSoup(response)\n",
      "                \n",
      "                title = soup.find('div',{'class':'news_story'}).find('h1').text\n",
      "                \n",
      "                content = soup.find('div',{'class':'story_content'}).findAll('p')[1:-1]                \n",
      "                for a in content:\n",
      "                    con = con + a.text.replace('\\n','').replace('\\r','').strip()\n",
      "\n",
      "                time_clean = soup.find('div',{'id':'reporter_info'}).find('p').text.encode('utf8')\n",
      "                date = time_clean.strip().replace('\u5e74','/').replace('\u6708','/').replace('\u65e5','').replace('\\n','').replace(' ','')\n",
      "                \n",
      "                content_kw = soup.find('div',{'class':'story_content'}).findAll('p')[-1]\n",
      "                keywords = content_kw.text.replace('\\n',' ')\n",
      "                \n",
      "                photo = soup.find('div',{'class':'autozoom'}).findAll('img')\n",
      "                photo_link = photo[0]['src']\n",
      "                \n",
      "                data_insert(title, date, con, news_link, filename, keywords, photo_link)\n",
      "                time.sleep(5)\n",
      "                print '.',\n",
      "            except:\n",
      "                retry_dic[news_link]=1\n",
      "                #print news_link\n",
      "        f.close()\n",
      "        retry_get_info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \u5b58\u5165mongoDB\n",
      "def data_insert (title, date, content, news_link, filename, keywords, photo_link):\n",
      "    con = Connection() \n",
      "    db = con.news\n",
      "    nownews_news = db.nownews\n",
      "\n",
      "    doc_catalog = filename.split('_')[0]\n",
      "    doc_id = news_link.split('n/')[1].replace('/','')\n",
      "    \n",
      "    for a in nownews_news.find(): \n",
      "        doc_id_dic[a.values()[7]] = 1\n",
      "\n",
      "    if doc_id not in doc_id_dic:\n",
      "        data = {\"_id\":doc_id,\n",
      "                    \"title\":title,\n",
      "                   \"date\":date,\n",
      "                   \"catalog\":doc_catalog,\n",
      "                   \"from\":\"nownews\",\n",
      "                   \"news_link\":news_link,\n",
      "                   \"content\":content,\n",
      "                   \"keywords\":keywords,\n",
      "                   \"photo_link\":photo_link}\n",
      "        nownews_news.insert(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get_catalog_linklist('http://www.nownews.com/','nownews/')\n",
      "#get_news_contents('nownews/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}